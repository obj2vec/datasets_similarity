{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wordpair:\n",
    "    word1 = None\n",
    "    word2 = None\n",
    "    \n",
    "    def __init__(self, word1, word2):\n",
    "        self.word1 = word1\n",
    "        self.word2 = word2\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.word1 + ',' + self.word2)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, type(self)) and hash(self) == hash(other)\n",
    "    \n",
    "    def inverse(self):\n",
    "        return Wordpair(self.word2, self.word1)\n",
    "    \n",
    "    def to_list_of_strings(self):\n",
    "        return [self.word1, self.word2]\n",
    "    \n",
    "    def to_string(self, separator=','):\n",
    "        return self.word1 + separator + self.word2\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.to_string()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index == 0:\n",
    "            return self.word1\n",
    "        elif index == 1:\n",
    "            return self.word2\n",
    "        else:\n",
    "            raise IndexError\n",
    "    \n",
    "    def word_in_pair(self, word):\n",
    "        return self.word1 == word or self.word2 == word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Labelpair(Wordpair):\n",
    "    \"\"\"\n",
    "    Labels are expected to be POS (part of speech)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, label1, label2):\n",
    "        super().__init__(label1, label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \"\"\"\n",
    "    Vault for all word similarities with labels\n",
    "    \n",
    "    \"\"\"\n",
    "    data_similarity = None\n",
    "    data_labels = None\n",
    "    \n",
    "    def __init__(self, similarities=None, labels=None):\n",
    "        if similarities is None:\n",
    "            self.data_similarity = {}\n",
    "        else:\n",
    "            self.data_similarity = similarities\n",
    "        \n",
    "        if labels is None:\n",
    "            self.data_labels = {}\n",
    "        else:\n",
    "            self.data_labels = labels\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_similarity)\n",
    "        \n",
    "    def add(self, wordpair, labelpair, value):\n",
    "        if type(wordpair) == tuple and type(labelpair) == tuple:\n",
    "            return self.add(Wordpair(wordpair[0], wordpair[1]), Wordpair(labelpair[0], labelpair[1]), value)\n",
    "        \n",
    "        self.data_similarity[wordpair] = value\n",
    "        self.data_labels[wordpair] = labelpair\n",
    "        \n",
    "    def pop(self, wordpair):\n",
    "        if type(wordpair) == tuple or type(wordpair) == list:\n",
    "            return self.pop(Wordpair(wordpair[0], wordpair[1]))\n",
    "        \n",
    "        self.data_similarity.pop(wordpair)\n",
    "        self.data_labels.pop(wordpair)\n",
    "        \n",
    "    def get(self, wordpair):\n",
    "        if type(wordpair) == tuple or type(wordpair) == list:\n",
    "            return self.get(Wordpair(wordpair[0], wordpair[1]))\n",
    "        \n",
    "        return self.data_similarity[wordpair]\n",
    "        \n",
    "    def get_with_labels(self, wordpair):\n",
    "        if type(wordpair) == tuple or type(wordpair) == list:\n",
    "            return self.get_with_labels(Wordpair(wordpair[0], wordpair[1]))\n",
    "        \n",
    "        return [self.data_labels[wordpair].to_list_of_strings(), self.data_similarity[wordpair]]\n",
    "    \n",
    "    def to_list(self):\n",
    "        result = []\n",
    "        all_pairs = self.data_similarity.keys()\n",
    "        \n",
    "        for pair in all_pairs:\n",
    "            curr_list = pair.to_list_of_strings() + \\\n",
    "                self.data_labels[pair].to_list_of_strings() + \\\n",
    "                [float(self.data_similarity[pair])]\n",
    "            result.append(curr_list)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def get_all_pairs_with_word(self, word):\n",
    "        res = []\n",
    "        \n",
    "        for pair in self.data.data_similarity:\n",
    "            if pair.word_in_pair(word):\n",
    "                res.append(pair)\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\" \n",
    "    Base class for dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    path = None\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def load_data_to_memory(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoldenStandartDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Desribes arbitrary golden standart\n",
    "    \n",
    "    \"\"\"\n",
    "    standartized_label = \"standartized\"\n",
    "    data = None\n",
    " \n",
    "    def __init__(self, path, data=None):\n",
    "        super().__init__(path)\n",
    "        \n",
    "        if data is None:\n",
    "            self.data = Data()\n",
    "            self.load_data_to_memory()\n",
    "        else:\n",
    "            self.data = data\n",
    "            \n",
    "    def load_data_to_memory(self):\n",
    "        if self.standartized_label in self.path:\n",
    "            self.load_data_to_memory_standartized()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "    def load_data_to_memory_standartized(self):\n",
    "        \"\"\"\n",
    "        Read data from path in standartized form:\n",
    "        word1, word2, label1, label2, similarity value\n",
    "        \n",
    "        labels are expected to be POS (part of speech)\n",
    "        \n",
    "        \"\"\"\n",
    "        separator = ','\n",
    "        \n",
    "        with open(self.path, newline='\\n') as csv_file:\n",
    "            reader = csv.reader(csv_file, delimiter=separator)\n",
    "            for row in reader:\n",
    "                words = Wordpair(row[0], row[1])\n",
    "                labels = Labelpair(row[2], row[3])\n",
    "                sim_value = row[4]\n",
    "                \n",
    "                try:\n",
    "                    float(sim_value)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                \n",
    "                self.data.add(words, labels, sim_value)\n",
    "    \n",
    "    def write_data_to_file(self, filepath):\n",
    "        \"\"\"\n",
    "        Write data to filepath in standartized form:\n",
    "        word1, word2, label1, label2, similarity value\n",
    "\n",
    "        labels are expected to be POS (part of speech)\n",
    "\n",
    "        \"\"\"\n",
    "        separator = ','\n",
    "        \n",
    "        with open(filepath, 'w+', newline='\\n') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=separator)\n",
    "            writer.writerow([\"word1\", \"word2\", \"label1\", \"label2\", \"sim_value\"])\n",
    "            writer.writerows(self.data.to_list())\n",
    "    \n",
    "    def find_asymm_pairs(self, printing=True):\n",
    "        \"\"\"\n",
    "        Find asymm pairs like (w1, w2, v1) & (w2, w1, v2), where v1 != v2\n",
    "        \n",
    "        \"\"\"\n",
    "        arr = [key for key in self.data.data_similarity.keys()]\n",
    "        reversed_arr = [key.inverse() for key in self.data.data_similarity.keys()]\n",
    "        \n",
    "        direct_set = set(arr)\n",
    "        reversed_set = set(reversed_arr)\n",
    "        \n",
    "        intersected_keys = list(set.intersection(direct_set, reversed_set))\n",
    "\n",
    "        sym_keys = []\n",
    "\n",
    "        for key in intersected_keys:\n",
    "            if key.inverse() in sym_keys or key.inverse() == key:\n",
    "                continue\n",
    "            sym_keys.append(key)\n",
    "\n",
    "        if len(sym_keys) == 0 and printing:\n",
    "            print('no asymmetrical pairs')\n",
    "\n",
    "        for key in sym_keys:\n",
    "            val1 = self.data.data_similarity[key]\n",
    "            val2 = self.data.data_similarity[key.inverse()]\n",
    "            if printing:\n",
    "                print(str(key) + ',' + str(val1), ';', str(key.inverse()) + ',' + str(val2))\n",
    "\n",
    "        return sym_keys\n",
    "    \n",
    "    def find_all_symm_keys(self):\n",
    "        \"\"\"\n",
    "        Find symm pairs like (w1, w2) & (w2, w1)\n",
    "        \n",
    "        \"\"\"\n",
    "        arr = [key for key in self.data.data_similarity.keys()]\n",
    "        reversed_arr = [key.inverse() for key in self.data.data_similarity.keys()]\n",
    "        \n",
    "        direct_set = set(arr)\n",
    "        reversed_set = set(reversed_arr)\n",
    "        \n",
    "        intersected_keys = list(set.intersection(direct_set, reversed_set))\n",
    "        \n",
    "        return intersected_keys\n",
    "    \n",
    "    def calculate_POS_distribution(self):\n",
    "        \"\"\"\n",
    "        Calculate words part of speech distribution\n",
    "        \n",
    "        \"\"\"\n",
    "        speech_parts = {}\n",
    "        \n",
    "        for label_pair in self.data.data_labels.values():\n",
    "            for label in label_pair.to_list_of_strings():\n",
    "                if label not in speech_parts:\n",
    "                    speech_parts[label] = 1\n",
    "                else:\n",
    "                    speech_parts[label] += 1\n",
    "\n",
    "        for part in speech_parts:\n",
    "            speech_parts[part] /= (len(self.data.data_labels) * 2)\n",
    "\n",
    "        return speech_parts\n",
    "    \n",
    "    def get_all_words_and_labels(self):\n",
    "        \"\"\"\n",
    "        Get list of all unique words & list of their labels\n",
    "        \n",
    "        \"\"\"\n",
    "        all_words = []\n",
    "        all_words_labels = []\n",
    "        \n",
    "        for pair in self.data.data_labels.keys():\n",
    "            label_pair = self.data.data_labels[pair]\n",
    "            \n",
    "            for word_idx in [0, 1]:\n",
    "                if pair[word_idx] not in all_words:\n",
    "                    all_words.append(pair[word_idx])\n",
    "                    all_words_labels.append(label_pair[word_idx])\n",
    "            \n",
    "        return all_words, all_words_labels\n",
    "    \n",
    "    \n",
    "    def train_test_split(self, test_ratio, only_test_words_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Split all pairs to train & test with respect to original POS distribution\n",
    "        \n",
    "        Parameters:\n",
    "        test_ratio : part of pair that goes to test\n",
    "        only_test_words_ratio : part of words that exists only in test\n",
    "        \n",
    "        Example:\n",
    "        \n",
    "        imagine original dataset with 1000 pairs & 800 unique words\n",
    "        test_ratio=0.3 means that test should contain 300 pairs\n",
    "        only_test_words_ratio=0.1 means that test should contain 80 words existing only in test\n",
    "        \n",
    "        \"\"\"\n",
    "        POS_distr = self.calculate_POS_distribution()\n",
    "        \n",
    "        all_words, all_words_labels = self.get_all_words_and_labels()\n",
    "        \n",
    "        all_words_size = len(all_words)\n",
    "        only_test_words_size = int(all_words_size * only_test_words_ratio)\n",
    "        test_size = int(len(self.data.data_similarity) * test_ratio)\n",
    "        \n",
    "        test_dict = Data()\n",
    "        \n",
    "        for idx in range(only_test_words_size):\n",
    "            if (len(test_dict) >= test_size):\n",
    "                break\n",
    "                \n",
    "            curr_label = np.random.choice(list(POS_distr.keys()), p=list(POS_distr.values()))\n",
    "            possible_words_idxs = np.where(np.array(all_words_labels) == curr_label)[0]\n",
    "            curr_word_idx = np.random.choice(possible_words_idxs)\n",
    "            curr_word = all_words[curr_word_idx]\n",
    "            all_words.pop(curr_word_idx)\n",
    "            all_words_labels.pop(curr_word_idx)\n",
    "            \n",
    "            for pair in self.data.data_similarity:\n",
    "                if pair.word_in_pair(curr_word):\n",
    "                    test_dict.add(pair, self.data.data_labels[pair], self.data.data_similarity[pair])\n",
    "            \n",
    "        gap_size = test_size - len(test_dict)\n",
    "                \n",
    "        if gap_size > 0:\n",
    "\n",
    "            while len(test_dict) < test_size:\n",
    "                curr_label = np.random.choice(list(POS_distr.keys()), p=list(POS_distr.values()))\n",
    "                possible_words_idxs = np.where(np.array(all_words_labels) == curr_label)[0]\n",
    "                curr_word_idx = np.random.choice(possible_words_idxs)\n",
    "                curr_word = all_words[curr_word_idx]\n",
    "\n",
    "                potencial_pairs = []\n",
    "                for pair in self.data.data_similarity:\n",
    "                    if pair.word_in_pair(curr_word):\n",
    "                        potencial_pairs.append(pair)\n",
    "                \n",
    "                if len(potenial_pairs <= 1):\n",
    "                    continue\n",
    "\n",
    "                pair = np.random.choice(potencial_pairs)\n",
    "                \n",
    "                if len(self.get_all_pairs_with_word(self, pair[0])) <= 1 or \\\n",
    "                len(self.get_all_pairs_with_word(self, pair[1])) <= 1:\n",
    "                    continue\n",
    "\n",
    "                if pair not in test_dict.data_similarity and len(get_all_pairs_with_word(curr_word)):\n",
    "                    test_dict.add(pair, self.data.data_labels[pair], self.data.data_similarity[pair])\n",
    "        \n",
    "        train_dict = Data()\n",
    "        for pair in self.data.data_similarity:\n",
    "            if pair not in test_dict.data_similarity:\n",
    "                train_dict.add(pair, self.data.data_labels[pair], self.data.data_similarity[pair])\n",
    "        \n",
    "        return GoldenStandartDataset(self.path + \"train\", train_dict), GoldenStandartDataset(self.path + \"test\", test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimLex999Dataset(GoldenStandartDataset):\n",
    "\n",
    "    def __init__(self, path=\"./SimLex-999.txt\"):\n",
    "        super().__init__(path)\n",
    "    \n",
    "    def load_data_to_memory(self):\n",
    "        if self.path.endswith(self.standartized_label):\n",
    "            super().load_data_to_memory()\n",
    "        else:\n",
    "            separator = '\\t'\n",
    "            \n",
    "            with open(self.path, newline='\\n') as csv_file:\n",
    "                reader = csv.reader(csv_file, delimiter=separator)\n",
    "                \n",
    "                line_idx = 0\n",
    "                for row in reader:\n",
    "                    if (line_idx == 0):\n",
    "                        line_idx += 1\n",
    "                        continue\n",
    "                    \n",
    "                    words = Wordpair(row[0], row[1])\n",
    "                    labels = Labelpair(row[2], row[2])\n",
    "                    sim_value = float(row[3])\n",
    "\n",
    "                    self.data.add(words, labels, sim_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSim353Dataset(GoldenStandartDataset):\n",
    "\n",
    "    def __init__(self, path=\"./combined.csv\"):\n",
    "        super().__init__(path)\n",
    "    \n",
    "    def load_data_to_memory(self):\n",
    "        if self.path.endswith(self.standartized_label):\n",
    "            super().load_data_to_memory()\n",
    "        else:\n",
    "            separator = ','\n",
    "            \n",
    "            with open(self.path, newline='\\n') as csv_file:\n",
    "                reader = csv.reader(csv_file, delimiter=separator)\n",
    "                \n",
    "                line_idx = 0\n",
    "                for row in reader:\n",
    "                    if (line_idx == 0):\n",
    "                        line_idx += 1\n",
    "                        continue\n",
    "                    \n",
    "                    words = Wordpair(row[0], row[1])\n",
    "                    labels = Labelpair(\"n\", \"n\")\n",
    "                    sim_value = float(row[2])\n",
    "\n",
    "                    self.data.add(words, labels, sim_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MENDataset(GoldenStandartDataset):\n",
    "    def __init__(self, path=\"./MEN_dataset_lemma_form_full\"):\n",
    "        super().__init__(path)\n",
    "    \n",
    "    def load_data_to_memory(self):\n",
    "        if self.path.endswith(self.standartized_label):\n",
    "            super().load_data_to_memory()\n",
    "        else:\n",
    "            separator = ' '\n",
    "            \n",
    "            with open(self.path, newline='\\n') as csv_file:\n",
    "                reader = csv.reader(csv_file, delimiter=separator)\n",
    "\n",
    "                for row in reader:\n",
    "                    \n",
    "                    words = Wordpair(row[0][:-2], row[1][:-2])\n",
    "                    labels = Labelpair(row[0][-1], row[1][-1])\n",
    "                    sim_value = float(row[2])\n",
    "\n",
    "                    self.data.add(words, labels, sim_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingEvaluator:\n",
    "    golden_data = None\n",
    "    golden_data_name = None\n",
    "    fit_data = None\n",
    "    dist_func = None\n",
    "    \n",
    "    def __init__(self, golden_path=None, golden_standart=None):\n",
    "        if golden_path is None and golden_standart is None:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if golden_standart is not None:\n",
    "            golden_data_name = golden_standart\n",
    "            if golden_standart == \"SimLex-999\":\n",
    "                self.golden_data = SimLex999Dataset()\n",
    "            elif golden_standart == \"WordSim-353\":\n",
    "                self.golden_data = WordSim353Dataset()\n",
    "            elif golden_standart == \"MEN\":\n",
    "                self.golden_data = MENDataset()\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        \n",
    "        if golden_path is not None:\n",
    "            self.golden_data = GoldenStandartDataset(golden_path)\n",
    "        self.fit_data = Data()\n",
    "            \n",
    "    def fit(self, embeddings, dist_func=None):\n",
    "        \"\"\"\n",
    "        Saves embeddings to evaluate\n",
    "        \n",
    "        Parameters:\n",
    "        embeddings should support word indexing, like that:\n",
    "        embeddings['word'] = word embedding\n",
    "        \n",
    "        \"\"\"\n",
    "        if dist_func is None:\n",
    "            self.dist_func = distance.cosine\n",
    "        else:\n",
    "            self.dist_func = dist_func\n",
    "        self.fit_data = embeddings\n",
    "        \n",
    "    def evaluate(self, method='spearman'):\n",
    "        \"\"\"\n",
    "        Evaluate embeddings quality\n",
    "        \n",
    "        Parameters:\n",
    "        method='spearman' : calculate rank correlation between golden standart & embeddings\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.fit_data is None:\n",
    "            raise NotImplementedError\n",
    "        if method == 'spearman':\n",
    "            golden_scores = []\n",
    "            fitted_scores = []\n",
    "            for pair in self.golden_data.data.data_similarity:\n",
    "                golden_scores.append(self.golden_data.data.data_similarity[pair])\n",
    "                emb1 = self.fit_data[pair[0]]\n",
    "                emb2 = self.fit_data[pair[1]]\n",
    "                \n",
    "                if type(emb1) == map:\n",
    "                    emb1 = list(emb1)\n",
    "                if type(emb2) == map:\n",
    "                    emb2 = list(emb2)\n",
    "                if len(emb1) == 0 or len(emb2) == 0:\n",
    "                    # no such word in given embeddings\n",
    "                    raise NotImplementedError\n",
    "\n",
    "                fitted_scores.append(self.dist_func(np.array(emb1), np.array(emb2)))\n",
    "                \n",
    "            corr = stats.spearmanr(np.array(golden_scores), np.array(fitted_scores))\n",
    "            print(\"Spearman correlation on {}: {}\".format(self.golden_data_name, corr))\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex = SimLex999Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strange,sly,2.07 ; sly,strange,1.97\n"
     ]
    }
   ],
   "source": [
    "simlex_asymm = simlex.find_asymm_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for asymm_pair in simlex_asymm:\n",
    "    simlex.data.pop(asymm_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no asymmetrical pairs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simlex.find_asymm_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex.write_data_to_file(\"./SimLex-999-standartized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex_train, simlex_test = simlex.train_test_split(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex_train.write_data_to_file(\"./SimLex-999-train-standartized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex_test.write_data_to_file(\"./SimLex-999-test-standartized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "699"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simlex_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simlex_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim = WordSim353Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money,bank,8.5 ; bank,money,8.12\n"
     ]
    }
   ],
   "source": [
    "wordsim_asymm = wordsim.find_asymm_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for asymm_pair in wordsim_asymm:\n",
    "    wordsim.data.pop(asymm_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no asymmetrical pairs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsim.find_asymm_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim.write_data_to_file(\"./WordSim-353-standartized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim_train, wordsim_test = wordsim.train_test_split(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim_train.write_data_to_file(\"./WordSim-353-train-standartized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim_test.write_data_to_file(\"./WordSim-353-test-standartized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordsim_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordsim_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = MENDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no asymmetrical pairs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men.find_asymm_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "men.write_data_to_file(\"./MEN-standartized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_train, men_test = men.train_test_split(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_train.write_data_to_file(\"./MEN-train-standartized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_test.write_data_to_file(\"./MEN-test-standartized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(men_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(men_test.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
